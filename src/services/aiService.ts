import OpenAI from 'openai';
import { Stream } from 'openai/streaming';
import type { ChatMessage } from '@/types';
import { useUIStateStore } from '@/stores/uiStateStore';

// Placeholder until actual type is defined in src/types/index.ts
// interface ChatMessage {
//   id?: string; // id might be optional if it's generated by the DB
//   role: 'user' | 'assistant' | 'system';
//   content: string;
//   timestamp?: Date; // timestamp might be optional before saving to DB
// }

interface OpenRouterCompletionParams {
  systemPrompt: string;
  chatHistory: ChatMessage[];
  userMessage: string;
  apiKey: string;
  model: string;
  siteUrl?: string; // Recommended by OpenRouter for rate limiting
  appName?: string; // Recommended by OpenRouter for rate limiting
}

export const getOpenRouterCompletion = async ({
  systemPrompt,
  chatHistory,
  userMessage,
  apiKey,
  model,
  siteUrl = 'http://localhost:5173', // Default or get from env
  appName = 'OpenRouter Dialog App', // Default or get from env
}: OpenRouterCompletionParams): Promise<Stream<OpenAI.Chat.Completions.ChatCompletionChunk>> => {
  const { setGlobalLoading, setGlobalError } = useUIStateStore.getState();
  
  setGlobalLoading(true);
  setGlobalError(null);

  const openai = new OpenAI({
    baseURL: 'https://openrouter.ai/api/v1',
    apiKey: apiKey,
    dangerouslyAllowBrowser: true,
    defaultHeaders: {
      'HTTP-Referer': siteUrl,
      'X-Title': appName,
    },
  });

  const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
    { role: 'system', content: systemPrompt },
    ...chatHistory.map((msg) => ({
      role: msg.role as 'user' | 'assistant' | 'system',
      content: msg.content,
    })),
    { role: 'user', content: userMessage },
  ];

  try {
    const stream = await openai.chat.completions.create({
      model: model,
      messages: messages,
      stream: true,
    });
    // Note: setGlobalLoading(false) will be handled by the calling function 
    // after the stream is fully processed or if an error occurs during streaming.
    // For an immediate loading(false) after request initiation (not recommended for streams):
    // setGlobalLoading(false); 
    return stream;
  } catch (error: any) {
    console.error('Error fetching completion from OpenRouter:', error);
    let errorMessage = 'Failed to get completion from OpenRouter.';
    if (error instanceof OpenAI.APIError) {
      errorMessage = `OpenRouter API Error: ${error.status} ${error.name} ${error.message}`;
    }
    setGlobalError(errorMessage);
    setGlobalLoading(false);
    throw new Error(errorMessage); // Re-throw to allow calling function to handle
  }
};

// Example of how to use the stream (for testing/later implementation):
/*
async function main() {
  const { setGlobalLoading, setGlobalError } = useUIStateStore.getState();
  try {
    // No need to call setGlobalLoading/Error here, getOpenRouterCompletion handles it
    const stream = await getOpenRouterCompletion({
      systemPrompt: "You are a helpful assistant.",
      chatHistory: [
        { role: 'user', content: 'Hello there!' },
        { role: 'assistant', content: 'Hi! How can I help you?' }
      ],
      userMessage: "What is the weather like today?",
      apiKey: "YOUR_OPENROUTER_API_KEY", // Replace with your actual key
      model: "openai/gpt-3.5-turbo", // Replace with your desired model
    });

    // Set loading to false once the stream starts, or after it ends.
    // For this example, we assume the main loading is for the request initiation.
    // More granular loading (e.g. for message chunks) would be handled in the UI component.
    setGlobalLoading(false); 

    for await (const chunk of stream) {
      process.stdout.write(chunk.choices[0]?.delta?.content || "");
    }
    console.log('\nStream finished.');
  } catch (error) {
    console.error('Error in main function:', error);
    // setGlobalError is already called by getOpenRouterCompletion if the error originated there
    // If an error occurs during stream processing itself, you might want to set it here:
    // if (!useUIStateStore.getState().globalError) { // Avoid overwriting specific API error
    //   setGlobalError(error instanceof Error ? error.message : 'Error processing stream.');
    // }
    // setGlobalLoading(false); // Ensure loading is off if stream processing fails
  }
}

// main(); // Uncomment to test locally, ensure you have an API key
*/ 